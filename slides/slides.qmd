---
title: "Working with NAs and Missing Data"
author: "Andrew Ghazi"
institute: "Core for Computational Biomedicine"
date: last-modified
format: 
    revealjs:
      theme: [default, my_theme.scss]
      width: 1600
      height: 900
      incremental: false 
      transition: fade
      background-transition: fade
      transition-speed: fast
      chalkboard: true
      slide-number: c/t
      controls: true
      controls-layout: edges
      controls-tutorial: true
        
title-slide-attributes:
  data-background-image: images/HMS_DBMI_Logo.png
  data-background-size: 25%
  data-background-position: 10% 10%
  
---

# Intro
```{r setup}
#| message: false
library(fastverse)
library(tinyplot)
# library(ggplot2)
library(ggbeeswarm)
set.seed(123)

ow = "#F2F2F2"

ow_rect = element_rect(fill  = ow, 
                       color = ow)

ow_bg = theme(panel.background = ow_rect,
              plot.background  = ow_rect)
```

## Who we are

> The Core for Computational Biomedicine (CCB) is a multi-disciplinary team of computational and quantitative scientists that collaborates with HMS Quad-based researchers to develop tools, platforms, and other data science and computational solutions that broadly enable biomedical discovery and innovation.

* Collaborative projects
* Office hours
* Workshops

:::{#headshots layout-ncol=1 style="text-align: center"}

![Andrew Ghazi, PhD](images/Andrew_Ghazi_003_sq_sm2.JPG){height=220}

<!-- + other CCB folks -->
:::

## CCB R/Stats Office Hours

::::{.columns}

:::{.column width="55%"}
* Quick help on R/Stats and CompBio
* Book Zoom any day or in-person on Wednesdays
* Examples:
  * Data wrangling / plotting messy health database exports
  * Prognostic modeling of surgery patients from imaging features with random forests
  * R project installation & organization
  * Running AlphaFold3 on O2
:::

:::{.column width="45%"}
![](images/oh.png)

<!-- Other figure elements to possibly include: -->
<!-- * survey / Kaplan Meier plot -->
<!-- * GWAS -->
<!-- * UMAP -->
:::

::::

:::{.absolute left=0 top=770}
Link in my email signature or at [https://dbmi.hms.harvard.edu/about-dbmi/core-computational-biomedicine/office-hours](https://dbmi.hms.harvard.edu/about-dbmi/core-computational-biomedicine/office-hours) 
:::

## Who you are

Prerequisites:

* Seminar: none
* Hands-on workshop: R and the packages below

```{r}
#| eval: false
#| echo: true

pkgs <- c("ggplot2", "dplyr", "mice", "ggmice", "brms")

install.packages(pkgs, Ncpus = 4)
```

## Missing data and NAs

:::{.column width=60%}
* ubiquitous and important problem
* decent methods exist
* methods aren't challenging, routine for ~15 years


:::{.fragment}
* **BUT** a thorough overview requires advanced methods 

![](images/m2.png){width=80% fig-align="center" style="filter: drop-shadow(0 0 0.75rem grey);"}
:::

:::

:::{.absolute left=970 top=40 width=50%}
```{r}

ins_na = \(x) {
  x[sample(length(x), floor(n/2))] = NA
  x
}

n = 200
set.seed(123)
data.table(age = floor(runif(n, 30,80)) |> ins_na(),
           income = floor(rexp(n, 1/48)) |> ins_na(),
           `5y_outcome` = sample(0:1, size = n, T) |> ins_na())[1:5,] |> 
  print(row.names = FALSE)
```
:::

![](images/dg_tmp.png){.absolute left=1000 top=440 width=40%} 

## Outline 

* Seminar (1 hour)
  * Concepts
  * Examples
* Hands-on workshop  in R (1 hour)

![](images/2025-06-11_15-35-18.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" .absolute left=800 top=100 width=40%}
![](images/no.svg){fig-alt="" .absolute left=770 top=50 .fragment}

# Concepts 

## Degrees of uncertainty

* Fill in the blank: 
    * "Harvard Uni_ersity"
    * "_ight" 

:::{.fragment}
* "Imputation" - the act of filling in blanks
:::

## Degrees of uncertainty 

::::{.columns}

:::{.column width=50%}
* Context informs uncertainty when imputing
* If the uncertainty is *sufficiently* small, it can be ignored
* What will you use imputed values for?
    * Just reporting them? Uncertainty is less valuable/important.
    * As input to a predictive and/or probabilistic model? Accounting for uncertainty is **essential**.
:::

:::{.notes}
* People generally ascribe negative value to uncertainty
:::

:::{.column width=50%}
```{r}

set.seed(42)

n = 30
s = .06
d = data.frame(x = rnorm(n)) |> 
    mtt(y = rnorm(n, 1*x, sd = s),
        z = rnorm(n, 1*x, sd = 1))

tinytheme("clean2", 
    mar = c(4.5,4,1,1),
    bg = ow,
    cex.axis = 1.5)

par(mfrow = c(1,2))

plt(d$x, d$y, 
     xlab = "", 
     ylab = "", 
     ylim = range(c(d$y, d$z)), 
     pch = 19)

tinyplot(col = "firebrick2",
         add = TRUE,
         type = "lines",
      c(1,1),
      c(1+s*2, 1-s*2),
      lwd = 2)

# points(1, 1, col = "firebrick2", pch = 21, cex = 1.3)

plt(d$x, d$z, 
     xlab = "", 
     ylab = "", 
     ylim = range(c(d$y, d$z)), 
     pch = 19)

plt(col = "firebrick2",
    type = "lines",
    add = TRUE,
    c(1,1),
    c(1+1*2, 1-1*2),
    lwd = 2)

# points(1, 1, col = "firebrick2", bg = "white", pch = 21, cex = 1.3)

```
:::
:::

## Uses for imputed values

* *Are the imputed values the means or the ends?*
* Draw a pretty picture
* Downstream inference

<!-- ## Types of observations  -->

<!-- * Observation: `5.0` -->
<!-- * Observation with error: `5.0 ± 0.6` -->
<!-- * Observation with distribution: $SkewNormal(4,2,.8)$ -->

<!-- :::{.nonincremental} -->

<!-- * Random variables are harder to work with, but sometimes necessary. -->
<!-- <!-- * Non-gaussian uncertainty = even harder --> -->

<!-- ::: -->

<!-- :::{.notes} -->
<!-- "Random variables are a different type of thing." -->
<!-- ::: -->

<!-- ```{r} -->
<!-- #| fig-align: center -->

<!-- d = data.table(g = c("observed", "obs_with_err"), -->
<!--            x = c(5, 5), -->
<!--            y = c(3, 2))  -->

<!-- d2 = data.table(g = "obs_with_dist", -->
<!--                               x = seq(4, 6.5, length.out = 200)) |>  -->
<!--               mtt(y = .5*dgamma(x-4, 2.5, 2.5)+1) -->

<!-- d |> -->
<!--   ggplot(aes(x, y)) +  -->
<!--   geom_point(size = 4) +  -->
<!--   geom_segment(data = data.frame(y = 2, yend = 2, x = 4.4, xend = 5.6), -->
<!--                aes(x = x, y = y, xend = xend, yend = yend), -->
<!--                inherit.aes = FALSE) +  -->
<!--   geom_line(data = d2, -->
<!--             color = "grey20") +  -->
<!--   labs(y = NULL) +  -->
<!--   theme_light() +  -->
<!--   theme(text = element_text(size = 24)) +  -->
<!--   ow_bg +  -->
<!--   scale_y_continuous(breaks = 1:3, -->
<!--                      labels = c("obs_with_dist", "obs_with_err", "observed")) -->
<!-- ``` -->


## Pathological situations

* Obvious: Missing data → less information → imprecise estimates
* Insidious: Patterns of missingness can *induce* bias if ignored

## Types of missingness

:::{.column width=60%}
* Missing...
  * "...completely at random" (MCAR)
  * "...at random" (MAR)
  * "...not at random" (MNAR)
* These terms are famously confusing.
* Draw a directed acyclic graph (DAG) instead.
:::

:::{.fragment}

* Example: Studying, homework, and dogs.

![](images/2025-05-06_15-58-00.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" .absolute left=1250 top=0}

:::{.absolute left=900 width=22% top=50}
<small> Arrows = causal relationships </small>

<small> Dotted = latent truth </small>

<small> Starred = observed with NAs </small>
:::

:::

# Strategies

## Just drop them

* Only valid for MCAR, otherwise you'll get bias
* At the very least, discuss the possible consequences of this choice

## Multiple imputation

* Key idea: 
  1. Randomly impute missing values
  1. Fit a model
  1. Repeat 1-2 many times
  1. Aggregate results
  
* Key assumption: "ignorability"

![](images/ch01-miflow-1.png){.absolute top=100 left=800 fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" width=55%}

:::{.aside}
[*Flexible Imputation of Missing Data*](https://stefvanbuuren.name/fimd/), Stef van Buuren, [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)
:::

## "Ignorability"

* "Does the chance of a value going missing depend on the value itself?"
* If so, assumption violated.
* multiple imputation won't work.

:::{.notes}
Defining this concept precisely is detailed, but the first bullet gives the main ida.
:::

:::{.fragment}
* If you have other data that can provide information on the underlying values, speak to a statistician and mention the phrase *"Bayesian imputation"*.
:::

# Examples

## Highly accurate imputation - Genotype from markers

:::{.column width=60%}
* Input: microarray genotypes of ~100k-5M common marker SNPs
* Output: genotypes at query SNPs
* Output uncertainty: surprisingly low for common variants
* SHAPEIT - Segmented HAPlotype Estimation and Imputation Tools
:::

:::{.column width=40% .absolute top=100 left=1000}
![](images/2025-08-04_13-25-08.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);"}
![](images/2025-08-04_13-22-07.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);"}
:::

<!-- TODO: picture of a microarray -->

## Inaccurate imputation for pictures - single-cell protein

:::{.column width=60%}
* Elastic Net, XGBoost, AutoEncoders to impute 16 protein levels of single cells.
* Proof of concept. N = 8.
* MAE b/w 0.05-0.15 on a 0-1 scale.
* Imputed error fairly high → Lessened downstream utility
* Batch, instrument effects? Unclear.
:::

:::{.column width=40%}
![](images/2025-08-04_12-36-50.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);"}
![](images/2025-08-04_12-38-36.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);"}
:::

## COVID incidence 

> "... a model that allows for simultaneous modeling of the disease and missingness processes, and that incorporates information on spatial clustering of risk"

> "population relative risk estimates by race during the early part of the COVID-19 pandemic in Michigan were understated for non-white residents, compared to white residents, when cases missing race were dropped or had these values imputed using MI."

![](images/2025-07-01_13-17-22.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" width=35% fig-align="center"} 

:::{.aside}

*Modeling racial/ethnic differences in COVID-19 incidence with covariates subject to nonrandom missingness* - Trangucci 2023
:::

# End matter

## How to do missing data analysis in Python

## ~~How to do missing data analysis in Python~~

<big> **Don't do missing data analysis in Python. ** </big>

:::{.incremental}
* `scikit-learn` ignores uncertainty from noise and parameter estimation.
* That is statistical malpractice. You **WILL** get the wrong answer.
* `scikit-learn` is for machine learning, *not* probabilistic inference 
  * See also the infamous `sklearn.linear_model.LogisticRegression`, another wrong-by-default function
* [`miceforest`](https://miceforest.readthedocs.io/en/latest/) - possibly okay for small data if RFs are acceptable
:::

## Examples

* genotype impute
* scdropout
* COVID data
* find a good survey example



