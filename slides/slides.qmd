---
title: "Working with NAs and Missing Data"
author: "Andrew Ghazi"
institute: "Core for Computational Biomedicine"
date: September 24, 2025 
format: 
    revealjs:
      theme: [default, my_theme.scss]
      width: 1600
      height: 900
      incremental: false 
      transition: fade
      background-transition: fade
      transition-speed: fast
      chalkboard: true
      slide-number: c/t
      controls: true
      controls-layout: edges
      controls-tutorial: true
        
title-slide-attributes:
  data-background-image: images/HMS_DBMI_Logo.png
  data-background-size: 25%
  data-background-position: 10% 10%
---


# Preface

## Setup {visibility="hidden"}

<!-- Repo: https://github.com/ccb-hms/missing_data/tree/main/slides -->
<!-- Rendered output: https://ccb.connect.hms.harvard.edu/missing_data -->

```{r setup}
#| message: false
library(fastverse)
library(tinyplot)
# library(ggplot2)
# library(ggbeeswarm)
# library(ggmice)
set.seed(123)

ow = "#F2F2F2"

# mice colors: 
# mice::mdc(r = c("observed", "missing")) |> dput()
mblue = "#006CC2B3"
mred = "#B61A51B3"

ow_rect = ggplot2::element_rect(fill  = ow,
                                color = ow) 

ow_bg = ggplot2::theme_minimal() + 
  ggplot2::theme(panel.background = ow_rect,
                 plot.background  = ow_rect,
                 legend.background = ow_rect,
                 text = ggplot2::element_text(size = 25))

set.seed(42)

n = 30
s = .06
d = data.frame(x = rnorm(n)) |> 
  mtt(y = rnorm(n, 1*x, sd = s),
      z = rnorm(n, 1*x, sd = 1))

yr = range(c(d$y, d$z))
```

<!-- ## Who we are -->
<!-- Ludwig doing this slide -->

<!-- > The Core for Computational Biomedicine (CCB) is a multi-disciplinary team of computational and quantitative scientists that collaborates with HMS Quad-based researchers to develop tools, platforms, and other data science and computational solutions that broadly enable biomedical discovery and innovation. -->

<!-- * Collaborative projects -->
<!-- * Office hours -->
<!-- * Workshops -->

<!-- :::{#headshots layout-ncol=1 style="text-align: center"} -->

<!-- ![Andrew Ghazi, PhD](images/Andrew_Ghazi_003_sq_sm2.JPG){height=220} -->

<!-- <!-- + TODO other CCB folks --> -->
<!-- ::: -->

## CCB R/Stats Office Hours

::::{.columns}

:::{.column width="55%"}
* *Quick* help
* Book a timeslot, Zoom or in-person
* Examples:
  * Data wrangling / plotting messy health database exports
  * Prognostic modeling of surgery patients from imaging features with random forests
  * R project installation & organization
  * debugging AlphaFold3 on O2
:::

:::{.column width="45%"}
![](images/oh.png)

<!-- Other figure elements to possibly include: -->
<!-- * survey / Kaplan Meier plot -->
<!-- * GWAS -->
<!-- * UMAP -->
:::

::::

:::{.absolute left=0 top=770}
Link in my email signature or at [https://dbmi.hms.harvard.edu/about-dbmi/core-computational-biomedicine/office-hours](https://dbmi.hms.harvard.edu/about-dbmi/core-computational-biomedicine/office-hours) 
:::

## Prerequisites:

* Seminar: none
* Hands-on workshop: R and the packages below:

```{r}
#| eval: false
#| echo: true

pkgs <- c("ggplot2", "dplyr", "purrr", 
          "mice", "ggmice", "brms")

install.packages(pkgs, Ncpus = 4)
```



## Outline 

:::{.column width=70%}
* Seminar (1 hour)
  * Concepts
  * Strategies
  * Examples
  * No theory or math
* Hands-on workshop  in R (1 hour)

These slides:

[ccb.connect.hms.harvard.edu/missing_data](ccb.connect.hms.harvard.edu/missing_data)

:::

![](images/2025-06-11_15-35-18.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" .absolute left=1000 top=50 width=33%}
![](images/no.svg){fig-alt="" .absolute left=985 top=20 .fragment width=35%}

# Concepts 

## Missing data and NAs

:::{.column width=60%}
* NA = **N**ot **A**pplicable
  * *not* the same as NaN, `NULL`, etc
* ubiquitous and important problem
* established methods/tools ~15 years
* not challenging, but requires statistics foundation

<!-- :::{.fragment} -->
<!-- * **BUT** a thorough overview requires advanced methods  -->

<!-- ![](images/m2.png){width=80% fig-align="center" style="filter: drop-shadow(0 0 0.75rem grey);"} -->
<!-- ::: -->

:::

:::{.absolute left=970 top=40 width=50%}
```{r}

ins_na = \(x) {
  x[sample(length(x), floor(n/3))] = NA
  x
}

n = 200
set.seed(123)
ex_dat = data.table(allerg = sample(0:1, n, TRUE) |> ins_na(),
                    age = floor(runif(n, 30,80))     |> ins_na(),
                    income = floor(rexp(n, 1/48))    |> ins_na(),
                    `5y_outcome` = sample(0:1, n, T) |> ins_na())

ex_dat$income[1] = 20
ex_dat$allerg[2] = NA

ex_dat[1:5,-1] |> 
  print(row.names = FALSE)
```
:::

[<span style="color:#666666; font-style: italic;"> <small> Missing values in survey data </small> </span>]{.absolute left=1000 top=390}

![ ](images/dg_tmp.png){.absolute left=1000 top=460 width=40%} 

[<span style="color:#666666; font-style: italic;"> <small> Missing values in imaging data </small> </span>]{.absolute left=1000 top=870}

## Degrees of uncertainty

* Fill in the blank: 
    * "Harvard Uni_ersity"
    * "_ight" 

:::{.fragment}
* "Imputation" - using context to fill in the blanks
:::

:::{.incremental}

1. high or low uncertainty over missing values
1. degree of uncertainty impacted by context
:::

## Degrees of uncertainty 

::::{.columns}

:::{.column width=44%}
* Context informs uncertainty when imputing
* If the uncertainty is *sufficiently* small, it can be ignored
* Using imputed values for...?
    * Just reporting / visualizing. Uncertainty is less important.
    * As input to a downstream model. Accounting for uncertainty is **essential**.
:::

:::{.notes}
* People generally ascribe negative value to uncertainty
:::

:::{.column width=56%}
```{r}

tinytheme("clean2", 
          mar = c(2,2,1,1),
          bg = ow,
          family = "Arial",
          cex.axis = 1.25,
          col.axis = "#222222")

par(mfrow = c(1,2))

plt(d$x, d$y, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 2,
    ylim = yr)

tinyplot(col = mred, 
         add = TRUE,
         type = "lines",
         c(1,1),
         c(1+s*2, 1-s*2),
         lwd = 4)

# points(1, 1, col = "firebrick2", pch = 21, cex = 1.3)

plt(d$x, d$z, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 2,
    ylim = yr) 

plt(col = mred,
    type = "lines",
    add = TRUE,
    c(1,1),
    c(1+1*2, 1-1*2),
    lwd = 4)

# points(1, 1, col = "firebrick2", bg = "white", pch = 21, cex = 1.3)

```
:::
:::

## Imputation - "Filling in the blank"

::::{.columns}
:::{.column width=46%}
* Generate a new value to replace NAs
* Build a predictive model for variable with NAs from other variables

:::{.fragment fragment-index=1}
* Many pitfalls:
  * replacing with the mean ❌ 
  * replacing with one value ❌
  * replacing with unrealistic values ❌
:::
:::

:::{.column width=54% .fragment fragment-index=1}
```{r}
par(mfrow = c(1,2))

plt(d$x, d$z, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 2,
    ylim = yr)

my = mean(d$y)

nx = c(-2, -1,1)

plt(nx,
    rep(my, 3),
    add = TRUE,
    col = mred,
    cex = 3)

lm_res = lm(z ~ x, data = d)

plt(d$x, d$z, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 2,
    ylim = range(c(d$y, d$z)))

abline(a = lm_res$coefficients[1],
       b = lm_res$coefficients[2],
       col = mred)

# plt(type = "l",
#     col = mred,
#     )

plt(nx, 
    predict(lm_res, data.frame(x = nx)),
    col = mred,
    add = TRUE,
    cex = 3)
```
:::
::::

## Uses for imputed values

> *Are the imputed values the means or the ends?*

- Fill in empty regions on a figure
  - UMAP or image
- Downstream statistical inference
  - $P(\theta \mid Y_{imp})$

***Inference is much more demanding!***

## Undesirable effects of missing data

* Obvious: Missing data → less information → imprecise estimates

:::{.fragment}

* Subtle yet harmful: Patterns of missingness can *induce* bias if ignored

```{r}
#| fig-width: 15

# tinytheme("clean2",
#           mar = c(2,2,1,1),
#           bg = ow,
#           cex.axis = 1.25,
#           col.axis = "#222222",
#           mfrow = c(1,3))

tinytheme("clean2", 
          mar = c(2,2,4,1),
          bg = ow,
          family = "Arial",
          cex.axis = 1.5,
          cex.main = 2,
          col.main = "#222222",
          col.axis = "#222222")

par(mfrow = c(1,3)) # mfrow behaves oddly within tinytheme

set.seed(1) 

n = 150 

d2 = data.table(x = rnorm(n)) |> 
  mtt(latent = rnorm(n, mean = 1*x),
      eta_z =  1.5*(latent - x) + -2.5*x - .9, 
      eta_w = -1.5*(latent - x) + -2.5*x - .9,
      z_mis = as.logical(rbinom(n, 1, plogis(eta_z))),
      w_mis = as.logical(rbinom(n, 1, plogis(eta_w))),
      z = data.table::fcase(z_mis, NA_real_,
                            !z_mis, latent),
      w = data.table::fcase(w_mis, NA_real_,
                           !w_mis, latent))

# d2 |> ggplot(aes(x, latent)) + geom_point(aes(color = eta_z)) + scale_color_viridis_c()

xr2 = range(d2$x); yr2 = range(d2$latent)

parula = pals::parula(100)

plt(d2$x, d2$latent, 
    xlim = xr2, ylim = yr2,
    # col = parula[as.numeric(cut(breaks = 100, d2$eta_z))],
    col = "grey44",
    pch = 19,
    main = "latent truth",
    cex = 1.7)

abline(reg = lm(latent ~ x, data = d2), col = "grey10")

plt(d2$x, d2$z, 
     xlim = xr2, ylim = yr2,
     pch = 19,
     col = mblue,
    main = "missingness increases slope",
    cex = 1.7)

abline(reg = lm(z ~ x, data = d2),
       col = mblue)

zm = is.na(d2$z)

points(d2$x[zm], d2$latent[zm],
       pch = 1,
       col = mred,
       cex = 1.7)

plt(d2$x, d2$w, 
     xlim = xr2, ylim = yr2,
     pch = 19,
     col = mblue,
    main = "missingness decreases slope",
    cex = 1.7)

abline(reg = lm(w ~ x, data = d2),
       col = mblue)

wm = is.na(d2$w)

points(d2$x[wm], d2$latent[wm],
       pch = 1,
       col = mred,
       cex = 1.7)

tinytheme("clean2", 
          mar = c(2,2,1,1),
          bg = ow,
          cex.axis = 1.25,
          col.axis = "#222222")


```
:::

## Types of missingness

:::{.column width=60%}
* Missing...
  * "...completely at random" (MCAR)
  * "...at random" (MAR)
  * "...not at random" (MNAR)

:::{.fragment fragment-index=2}

* These terms are famously confusing.
* Draw a directed acyclic graph (DAG) instead.
:::

:::

![](images/2025-05-06_15-58-00.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" .absolute left=1250 top=-30}

:::{.absolute left=900 width=22% top=50}
<small> Arrows = causal relationships </small>

<small> Dotted = latent truth </small>

<small> Starred = observed with NAs </small>
:::

::: {data-id="box1" style="background-color: rgba(242, 242, 242, 0.5); backdrop-filter: blur(15px); width: 1600px; height: 720px; " .absolute left=0 top=240 .fragment .fade-out fragment-index=1}
:::

# Strategies

## Examining missingness

* Count observations with any NAs
* Tabulate features that tend to go missing together
* Check if NAs are associated with other variables

:::{layout-nrow=1}
![](images/ggmice_preview2.png){height=500}
![](images/ggmice_preview.png){height=500}
:::

```{r}
#| eval: false
#| echo: false
ggmice::ggmice(mice::boys,
               ggplot2::aes(age, hgt)) + 
    ggplot2::geom_point(size = 3, pch = 16) +
  ow_bg 
```


## Just drop them

* Only valid for MCAR, otherwise you'll get bias.
* At the very least:
  * Characterize dropped cases
  * Discuss the consequences of this choice

## Multiple imputation (MI)

* Key idea: 
  1. Randomly impute missing values
  1. Fit a model to the imputed dataset
  1. Repeat 1-2 many times
  1. Aggregate ("pool") results
  
* Key assumption: "ignorability"

![](images/ch01-miflow-1.png){.absolute top=100 left=750 fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" width=55%}

:::{.aside}
[*Flexible Imputation of Missing Data*](https://stefvanbuuren.name/fimd/), Stef van Buuren, [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)
:::

## Imputation model

::::{.columns} 

:::{.column width=45%}
* Predict NAs in a variable given others
* Most common: **Predictive mean matching** and/or **regression**, see `?mice::mice.impute.*`

:::{.fragment fragment-index=4}
* Predictive mean matching ✅✅
* LASSO, random forests, neural nets
* `mice` uses sane defaults, but always check and think.
:::


<!-- :::{.incremental} -->
<!-- * Mean of the non-NAs ❌ ❌[❌❌]{.absolute left=900 top=140} -->
<!-- * Model predictive mean ❌[❌]{.absolute left=1280 top=140} -->
<!-- * Multiple predictive mean + noise ✅[✅]{.absolute left=900 top=490} -->
<!-- * Predict + noise + $\theta$ uncertainty ✅✅[✅✅]{.absolute left=1280 top=490} -->
<!-- ::: -->
:::

<!-- TODO: FIMD figure 3.1 but with only one predictor for simplicity -->
```{r}
plot_d = function(l) {
  plt(d$x, d$z, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 1.8,
    ylim = yr)
  
  pd = .2
  
  cex_val = 1.33
  
  w = strwidth(l, cex = cex_val)
  h = strheight(l, cex = cex_val)
  
  x = -2.65
  y = 2.7
  
  plt(type = "rect",
      fill = 'grey90',
      add = TRUE,
      col = rgb(0,0,0,0),
      xmin = x - pd, xmax = x + w + pd,
      ymin = y - pd, ymax = y + h + pd)
  
  plt(type = "text",
      x = x + w/2, y = y + h/2,
      labels = l,
      col = "grey20",
      cex = cex_val,
      add = TRUE)

}

```

:::{.column width=55%}

:::{#figs layout-ncol=2}
```{r .fragment}
#| fig-height: 4
#| fig-width: 5 

plot_d("Mean of the non-NAs ❌❌")
my = mean(d$y)

nx = c(-2, -1,1)

plt(nx,
    rep(my, 3),
    add = TRUE,
    col = mred,
    cex = 1.8)

```

:::{.fragment fragment-index=1}

```{r}
#| fig-height: 4
#| fig-width: 5 

lm_res = lm(z ~ x, data = d)

plot_d("Model predictive mean ❌")

abline(a = lm_res$coefficients[1],
       b = lm_res$coefficients[2],
       col = mred,
       lwd = 1.8)

plt(nx, 
    predict(lm_res, data.frame(x = nx)),
    col = mred,
    add = TRUE,
    cex = 1.8)

```
:::

:::{.fragment fragment-index=2}
```{r}
#| fig-height: 4
#| fig-width: 5 

plot_d("Multiple predictive mean + noise ✅")

lm_res = lm(z ~ x, data = d)

abline(a = lm_res$coefficients[1],
       b = lm_res$coefficients[2],
       col = mred,
       lwd = 1.8)

set.seed(12)

plt(nx, 
    predict(lm_res, data.frame(x = nx)) + rnorm(3*4, sd = summary(lm_res)$sigma),
    col = mred,
    add = TRUE,
    cex = 1.8)

```
:::

:::{.fragment fragment-index=3}
```{r}
#| fig-height: 4
#| fig-width: 5 
plot_d("Predict + noise + θ uncertainty ✅✅")

# rstanarm::stan_glm(z ~ x, data = d) |> rstanarm::as_draws() |> head()

ddf = structure(list(`(Intercept)` = c(0.0471073690698166, 0.201052925741826, 
0.250345212091326, 0.198926964048982, 0.299822320222807, 0.0208228507399506
), x = c(0.93778290924944, 0.899190012816253, 0.918602409313972, 
0.743191159898737, 0.806488438565962, 0.959382089089642), sigma = c(0.757680208606442, 
0.973403819489721, 0.698234171286681, 0.782440029970045, 0.924800482357415, 
0.708375154404558)), row.names = c(NA, -6L), class = c( "data.frame"))

for (i in 1:4) {
  abline(a = ddf$`(Intercept)`[i],
       b = ddf$x[i],
       col = mred,
       lwd = 1.8)
  
  plt(nx, 
      rnorm(3, 
            mean = ddf$`(Intercept)`[i] + ddf$x[i]*nx, 
            sd = ddf$sigma[i]),
      col = mred,
      add = TRUE,
      cex = 1.8)
}
```
:::

:::


:::
::::

## Pooling

Aggregate estimates in a way that combines

1. imputation-wise uncertainty
1. uncertainty over imputations

```{.r}
?mice::pool()
```


## "Ignorability" assumption

* "Does the chance of a value going missing depend on the value itself or other information we don't have?"
* If so, assumption violated.
* standard MI practice generally won't work.

:::{.notes}
Defining this concept precisely is complicated, but the first bullet gives the main idea.
:::

<!-- :::{.fragment} -->
<!-- * If you have other data that can provide information on the underlying values, speak to a statistician and mention the phrase *"Bayesian imputation"*. -->
<!-- ::: -->

## Quiz 

```{r}
ex_dat[1:7,c(2:4,1)] |> print(row.names = FALSE)
```

[<span style="color:#666666; font-style: italic;"> <small> Missing values in survey data </small> </span>]{.absolute left=100 top=550}

![ ](images/dg_tmp.png){.absolute left=950 top=100 width=44%} 

[<span style="color:#666666; font-style: italic;"> <small> Missing values in imaging data </small> </span>]{.absolute left=950 top=550}

:::{.absolute top=600}
Discuss with your neighbor: Which of these are probably MCAR/MAR/NMAR and/or exhibit ignorable missingness?
:::

## Joint imputation

:::{.column width=65%}
* Use *informative auxiliary variables* to simultaneously:
  * impute NAs
  * infer missingness relationship 
* Generally requires:
  * informative auxiliary variables
  * bespoke modeling in a probabilistic programming language 
  * fairly large sample sizes
  * domain knowledge
:::

![](images/covid_dag.png){fig-alt="" .absolute left=1100 top=100 width=30%}

# Examples

## Highly accurate imputation - Genotype from markers

:::{.column width=60%}
* Input: microarray genotypes of ~100k-5M common marker SNPs
* Output: genotypes at query SNPs
* Output uncertainty: surprisingly low for common variants
* SHAPEIT - Segmented HAPlotype Estimation and Imputation Tools
:::

:::{.column width=40% .absolute top=100 left=1000}
![](images/2025-08-04_13-25-08.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);"}
![](images/2025-08-04_13-22-07.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);"}
:::

<!-- TODO: picture of a microarray -->

## Imprecise imputation for images - single-cell proteomics

:::{.column width=60%}
* Proof of concept. N = 8.
* Elastic Net, XGBoost, AutoEncoders to impute 16 proteins of single cells.
* MAE b/w 0.05-0.15 on a 0-1 scale.
* Imputed error fairly high → Lessened downstream utility
* Batch, instrument effects? Unclear.
:::

:::{.column width=40%}
![](images/2025-08-04_12-36-50.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);"}
![](images/2025-08-04_12-38-36.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);"}
:::

## COVID incidence 

> "... a model that allows for simultaneous modeling of the disease and missingness processes, and that incorporates information on spatial clustering of risk"

> "population relative risk estimates by race during the early part of the COVID-19 pandemic in Michigan were understated for non-white residents, compared to white residents, when cases missing race were dropped or had these values imputed using MI."

![](images/2025-07-01_13-17-22.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" width=35% fig-align="center"} 

:::{.aside}

*Modeling racial/ethnic differences in COVID-19 incidence with covariates subject to nonrandom missingness* - Trangucci 2023
:::

# End matter


## Ethics

> "The ethical statistical practitioner ... seeks to *understand and mitigate* known or *suspected* limitations, defects, or biases in the data ... and communicates potential impacts on the interpretation [or] conclusions...[and] Avoids compromising validity for expediency." ^[ [ASA Ethical Guidelines for Statistical Practice](https://www.amstat.org/your-career/ethical-guidelines-for-statistical-practice),  emphasis mine]

* Be transparent. Don't ignore things you know to be impactful.
* Do your due diligence. 
* Use the opportunity to advance statistical practice in your field.
* Ask for help if you need it.

# On to the workshop!

[ccb.connect.hms.harvard.edu/missing_data_workshop/](ccb.connect.hms.harvard.edu/missing_data_workshop/)

# Extra

## NA and friends

* "NA" - not applicable, not available, etc
* `NA` - logical constant in R
  * See `?NA` for `NA_real_`, `NA_character_`, etc
* NaN - 0/0 and so on
* `0` - Not the same as a missing value!
* Placeholders e.g. `age = 9999`
  * 🪦 

## {background-image="images/beach.png"} 

## How to do missing data analysis in Python

## ~~How to do missing data analysis in Python~~

<span style="font-size: 140%;"> **Don't do missing data analysis in Python. ** </span> 

:::{.incremental}
* MI ecosystem all but non-existent in Python.
* `scikit-learn` encourages single imputation ❌
  * statistical malpractice
* `scikit-learn` is for machine learning, *not* probabilistic inference 
  * See also the infamous `sklearn.linear_model.LogisticRegression`
* [`miceforest`](https://miceforest.readthedocs.io/en/latest/) - possibly okay for small data if computational burden of RFs is acceptable
:::

## Examples

* genotype impute
* scdropout
* COVID data
* find a good survey example



