---
title: "Modern Strategies for Working with Missing Data"
subtitle: "with applications in biomedical research"
author: "Andrew Ghazi"
institute: "Core for Computational Biomedicine"
date: September 24, 2025 
format: 
    revealjs:
      theme: [default, my_theme.scss]
      width: 1600
      height: 900
      incremental: false 
      transition: fade
      background-transition: fade
      transition-speed: fast
      chalkboard: true
      slide-number: c/t
      controls: true
      controls-layout: edges
      controls-tutorial: true
        
title-slide-attributes:
  data-background-image: images/HMS_DBMI_Logo.png
  data-background-size: 25%
  data-background-position: 10% 10%
---


# Preface

## Setup {visibility="hidden"}

<!-- Repo: https://github.com/ccb-hms/missing_data/tree/main/slides -->
<!-- Rendered output: https://ccb.connect.hms.harvard.edu/missing_data -->

```{r setup}
#| message: false
library(fastverse)
library(tinyplot)
# library(ggplot2)
# library(ggbeeswarm)
# library(ggmice)
set.seed(123)

ow = "#F2F2F2"

# mice colors: 
# mice::mdc(r = c("observed", "missing")) |> dput()
mblue = "#006CC2B3"
mred = "#B61A51B3"

ow_rect = ggplot2::element_rect(fill  = ow,
                                color = ow) 

ow_bg = ggplot2::theme_minimal() + 
  ggplot2::theme(panel.background = ow_rect,
                 plot.background  = ow_rect,
                 legend.background = ow_rect,
                 text = ggplot2::element_text(size = 25))

set.seed(42)

n = 30
s = .06
d = data.frame(x = rnorm(n)) |> 
  mtt(y = rnorm(n, 1*x, sd = s), # small sd
      z = rnorm(n, 1*x, sd = 1)) # big sd

yr = range(c(d$y, d$z))
```

## CCB R/Stats Office Hours

::::{.columns}

:::{.column width="55%"}
* Quick help on R/Stats & CompBio
* Book a timeslot, Zoom or in-person
* Examples:
  * Data wrangling / plotting messy health database exports
  * Prognostic modeling of surgery patients from imaging features with random forests
  * R project installation & organization
  * debugging AlphaFold3 on O2
:::

:::{.column width="45%"}
![](images/oh.png)
:::

::::

:::{.absolute left=0 top=770}
Link in my email signature or at [https://dbmi.hms.harvard.edu/about-dbmi/core-computational-biomedicine/office-hours](https://dbmi.hms.harvard.edu/about-dbmi/core-computational-biomedicine/office-hours) 
:::

## Prerequisites:

* Seminar: none
* Hands-on workshop: 
  * statistics basics
  * R basics
  * R and the packages below:

```{r}
#| eval: false
#| echo: true

pkgs <- c("ggplot2", "dplyr", "purrr", 
          "mice", "ggmice", "brms")

install.packages(pkgs, Ncpus = 4)
```



## Outline 

:::{.column width=70%}
* Seminar (1 hour)
  * Concepts
  * Strategies
  * Examples
  * No theory or math!
* Hands-on workshop  in R (1 hour)

These slides:

[ccb.connect.hms.harvard.edu/missing_data](ccb.connect.hms.harvard.edu/missing_data)

:::

![](images/2025-06-11_15-35-18.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" .absolute left=1000 top=50 width=33%}
![](images/no.svg){fig-alt="" .absolute left=985 top=20 .fragment width=35%}

# Concepts 

## Missing data and NAs

:::{.column width=60%}
* NA = **N**ot **A**pplicable
  * *not* the same as NaN, `NULL`, etc
* ubiquitous and important problem
* established methods/tools ~15 years
* not challenging, but requires statistics foundation

<!-- :::{.fragment} -->
<!-- * **BUT** a thorough overview requires advanced methods  -->

<!-- ![](images/m2.png){width=80% fig-align="center" style="filter: drop-shadow(0 0 0.75rem grey);"} -->
<!-- ::: -->

:::

:::{.absolute left=970 top=40 width=50%}
```{r}

ins_na = \(x) {
  x[sample(length(x), floor(n/3))] = NA
  x
}

n = 200
set.seed(123)
ex_dat = data.table(allerg = sample(0:1, n, TRUE) |> ins_na(),
                    age = floor(runif(n, 30,80))     |> ins_na(),
                    income = floor(rexp(n, 1/48))    |> ins_na(),
                    `5y_outcome` = sample(0:1, n, T) |> ins_na())

ex_dat$income[1] = 20
ex_dat$allerg[2] = NA

ex_dat[1:5,-1] |> 
  print(row.names = FALSE)
```
:::

[<span style="color:#666666; font-style: italic;"> <small> Missing values in survey data </small> </span>]{.absolute left=1000 top=390}

![](images/dg_tmp.png){.absolute left=1000 top=460 width=40%} 

[<span style="color:#666666; font-style: italic;"> <small> Missing values in imaging data. Image provided by HMS Single Cell Core.</small> </span>]{.absolute left=1000 top=870}

## Degrees of uncertainty

* Fill in the blank: 
    * "Harvard Uni_ersity"
    * "_ight" 

:::{.fragment}
* *Imputation* - using context to fill in the blanks
:::

:::{.incremental}
1. high or low uncertainty over missing values
1. degree of uncertainty impacted by context
:::

## Degrees of uncertainty 

::::{.columns}

:::{.column width=44%}
* Context informs uncertainty when imputing

<!-- :::{.fragment} -->
<!-- * If the uncertainty is *sufficiently* small, ignore it.  -->
<!-- * Using imputed values for...? -->
<!--     * Just reporting / visualizing. ✅ -->
<!--     * Input to scientific inference. ⚠️ -->
<!-- ::: -->

:::

:::{.notes}
* People generally ascribe negative value to uncertainty
:::

:::{.column width=56%}

<small> *Missing Y values at X = 1:* </small>

```{r}

tinytheme("clean2", 
          mar = c(2,2,1,1),
          family = "Arial",
          bg = ow,
          cex.axis = 1.5,
          col.axis = "#222222")

par(mfrow = c(1,2))

plt(d$x, d$y, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 2,
    ylim = yr)

lines(col = mred, 
      add = TRUE,
      c(1,1),
      c(1+s*2, 1-s*2),
      lend = "square", 
      lwd = 5)

# points(1, 1, col = "firebrick2", pch = 21, cex = 1.3)

plt(d$x, d$z, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 2,
    ylim = yr) 

lines(col = mred,
    add = TRUE,
    c(1,1),
    c(1+1*2, 1-1*2),
    lend = "butt",
    lwd = 5)

# points(1, 1, col = "firebrick2", bg = "white", pch = 21, cex = 1.3)

```
:::
:::

## Imputation - "Filling in the blank"

::::{.columns}
:::{.column width=46%}
* Generate a new value to replace NAs
* Build a predictive model for variable with NAs from other variables

:::{.fragment fragment-index=1}
* Many pitfalls:
  * replacing with the mean ❌ 
  * replacing with one value ❌
  * replacing with unrealistic values ❌
:::
:::


:::{.column width=54% .fragment fragment-index=1}

<small> *Given X = (-2, -1, 1), predict missing Y values:* </small>

```{r}
par(mfrow = c(1,2))

plt(d$x, d$z, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 2,
    ylim = yr)

legend(-2.6, 2.8,
       c("observed", "imputed Y"),
       col = c(mblue, mred),
       cex = 1.4,
       pt.cex = 2,
       bg = "#F2F2F2",
       box.col = "#222222",
       text.col = "#222222",
       pch = c(16, 16))

my = mean(d$y)

nx = c(-2, -1,1)

plt(nx,
    rep(my, 3),
    add = TRUE,
    col = mred,
    cex = 3)

lm_res = lm(z ~ x, data = d)

plt(d$x, d$z, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 2,
    ylim = range(c(d$y, d$z)))

abline(a = lm_res$coefficients[1],
       b = lm_res$coefficients[2],
       col = mred)

# plt(type = "l",
#     col = mred,
#     )

plt(nx, 
    predict(lm_res, data.frame(x = nx)),
    col = mred,
    add = TRUE,
    cex = 3)
```
:::
::::

## Uses for imputed values

> *Are the imputed values the means or the ends?*

- Fill in empty regions on a figure
  - UMAP or image
- Downstream scientific inference
  - $P(\theta \mid Y_{imp})$

***Inference is much more demanding!***

![](images/red_dotted.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" .absolute left=900 top=0 width=36%}
![](images/prop_ex.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" .absolute left=900 top=450 width=36%}

## Undesirable effects of missing data

* Obvious: Missing data → less information → imprecise estimates

:::{.fragment fragment-index=1}

* Subtle yet harmful: Patterns of missingness can *induce* bias if ignored

```{r}
#| eval: false

# tinytheme("clean2",
#           mar = c(2,2,1,1),
#           bg = ow,
#           cex.axis = 1.25,
#           col.axis = "#222222",
#           mfrow = c(1,3))

tinytheme("clean2", 
          mar = c(2,2,4,1),
          bg = ow,
          family = "Arial",
          cex.axis = 1.5,
          cex.main = 1.5,
          col.main = "#222222",
          col.axis = "#222222")

png("slides/images/patterns.png", width = 3*1500, height = 1250, units = "px",
    res = 400)
par(mfrow = c(1,4)) # mfrow behaves oddly within tinytheme

set.seed(1) 

n = 150

d2 = data.table(x = rnorm(n)) |> 
  mtt(latent = rnorm(n, mean = 1*x, sd = 1.2),
      eta_z =    4.5*(latent - x) + -7*(x+1), 
      eta_w = -1.5*(latent - x) + -3*x - 1.6,
      z_mis = as.logical(rbinom(n, 1, plogis(eta_z))) & (x < 0) | 
               (x < -.33 & latent > -.5) | 
               (x < -.5 & latent > -1) | 
        (x < -.7 & latent > -1.5 ),
      w_mis = as.logical(rbinom(n, 1, plogis(eta_w))),
      r_mis = !(latent < (-1*x + 1.33) & latent > (-1*x -.75)),
      z = data.table::fcase(z_mis, NA_real_,
                            !z_mis, latent),
      w = data.table::fcase(w_mis, NA_real_,
                           !w_mis, latent),
      r = data.table::fcase(r_mis, NA_real_,
                            !r_mis, latent))

# d2 |> ggplot(aes(x, latent)) + geom_point(aes(color = eta_z)) + scale_color_viridis_c()

xr2 = range(d2$x); yr2 = range(d2$latent)

plt(d2$x, d2$latent, 
    xlim = xr2, ylim = yr2,
    # col = parula[as.numeric(cut(breaks = 100, d2$eta_z))],
    col = "grey44",
    pch = 16,
    main = "latent truth",
    cex = 1.7)

abline(reg = lm(latent ~ x, data = d2), col = "grey10")

plt(d2$x, d2$z, 
     xlim = xr2, ylim = yr2,
     pch = 16,
     col = mblue,
    main = "missingness increases slope",
    cex = 1.7)

abline(reg = lm(z ~ x, data = d2),
       col = mblue)

zm = is.na(d2$z)

points(d2$x[zm], d2$latent[zm],
       pch = 1,
       col = mred,
       cex = 1.7)

legend(-2.3, 5.2,
       c("missing", "observed"),
       col = c(mred, mblue),
       cex = 1.4,
       pt.cex = 2,
       bg = "#F2F2F2",
       box.col = "#222222",
       text.col = "#222222",
       pch = c(1, 16))

plt(d2$x, d2$w, 
     xlim = xr2, ylim = yr2,
     pch = 16,
     col = mblue,
    main = "missingness decreases slope",
    cex = 1.7)

abline(reg = lm(w ~ x, data = d2),
       col = mblue)

wm = is.na(d2$w)

points(d2$x[wm], d2$latent[wm],
       pch = 1,
       col = mred,
       cex = 1.7)

plt(d2$x, d2$r, 
     xlim = xr2, ylim = yr2,
     pch = 16,
     col = mblue,
    main = "missingness flips slope",
    cex = 1.7)

abline(reg = lm(r ~ x, data = d2),
       col = mblue)

rm = is.na(d2$r)

points(d2$x[rm], d2$latent[rm],
       pch = 1,
       col = mred,
       cex = 1.7)
dev.off()
tinytheme("clean2", 
          mar = c(2,2,1,1),
          bg = ow,
          cex.axis = 1.25,
          col.axis = "#222222")


```


![](images/patterns.png)

:::


::: {data-id="box1" style="background-color: #F2F2F2; width: 1600px; height: 720px; " .absolute left=750 top=300 .fragment .fade-out fragment-index=2}
:::

## Types of missingness

:::{.column width=60%}
* Missing...
  * "...completely at random" (MCAR)
  * "...at random" (MAR)
  * "...not at random" (MNAR)

:::{.fragment fragment-index=2}

* These terms are famously confusing.
* Draw a directed acyclic graph (DAG) instead.
:::

:::

![](images/2025-05-06_15-58-00.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" .absolute left=1250 top=-30}

:::{.absolute left=900 width=22% top=50}
<small> Arrows: causal relationships </small>

<small> Dotted: latent true value </small>

<small> Starred: value with NAs </small>
:::

:::{.absolute top=875 left=1140}
<span style="color: #999999;"> <small> Diagram from *Statistical Rethinking* </small> </span>
:::

::: {data-id="box1" style="background-color: rgba(242, 242, 242, 0.5); backdrop-filter: blur(15px); width: 1600px; height: 1720px; " .absolute left=0 top=240 .fragment .fade-out fragment-index=1}
:::

# Strategies

## Examining missingness

* Count observations with any NAs
* Tabulate features that tend to go missing together
* Check if NAs are associated with other variables

:::{layout-nrow=1}
![](images/ggmice_preview2.png){height=500}
![](images/ggmice_preview.png){height=500}
:::

```{r}
#| eval: false 
#| echo: false

ggmice(boys, aes(age, hgt)) + 
  geom_point(size = 3, pch = 16) + 
  theme_minimal() +
  theme(panel.background = ow_rect, 
        panel.grid = element_blank(),
        plot.background = ow_rect,
        text = element_text(size = 18, family = "Arial")) +
  labs(y = "height")

ggsave("~/projects/missing_data/slides/images/ggmice_preview.png", h = 5, w = 7.5)

plot_pattern(boys)
```


## Just drop them

* Only valid for MCAR, otherwise you'll get bias.
* At the very least:
  * Characterize dropped cases
  * Discuss the consequences of this choice

## Multiple imputation (MI)

* Key idea: 
  1. Randomly impute missing values
  1. Fit a model to the imputed dataset
  1. Repeat 1-2 many times
  1. Aggregate ("pool") results
  
* Key assumption: "ignorability"

![](images/ch01-miflow-1.png){.absolute top=100 left=750 fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" width=55%}

:::{.aside}
[*Flexible Imputation of Missing Data*](https://stefvanbuuren.name/fimd/), Stef van Buuren, [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)
:::

## MI - Imputation model

::::{.columns} 

:::{.column width=45%}


![](images/ch01-miflow-1.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" width=75%}

::: {data-id="box1" style="border: 5px solid #FF0000; width: 220px; height: 270px; " .absolute left=20 top=115}
:::

* Predict NAs in a variable given others

[<small> *Given X = (-2, -1, 1), predict missing Y values:* </small>]{.absolute left=750 top=50}

:::{.fragment fragment-index=4}
* See `?mice::mice.impute.*`
* Predictive mean matching ✅✅
* LASSO, random forests, neural nets
* `mice` uses sane defaults, but always check and think.
:::

:::

<!-- FIMD figure 3.1 but with only one predictor for simplicity -->
```{r}
plot_d = function(l) {
  plt(d$x, d$z, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 1.8,
    ylim = yr)
  
  pd = .2
  
  cex_val = 1.33
  
  w = strwidth(l, cex = cex_val)
  h = strheight(l, cex = cex_val)
  
  x = -2.65
  y = 2.7
  
  plt(type = "rect",
      fill = 'grey90',
      add = TRUE,
      col = rgb(0,0,0,0),
      xmin = x - pd, xmax = x + w + pd,
      ymin = y - pd, ymax = y + h + pd)
  
  plt(type = "text",
      x = x + w/2, y = y + h/2,
      labels = l,
      col = "grey20",
      cex = cex_val,
      add = TRUE)

}

```

:::{.column width=55%}

:::{#figs layout-ncol=2}


```{r .fragment}
#| fig-height: 4
#| fig-width: 5 

# TODO prerender these, because the EKS server's R can't find the emojis in the font :( 

plot_d("Mean of the non-NAs ❌❌")

my = mean(d$y)

nx = c(-2, -1,1)

plt(nx,
    rep(my, 3),
    add = TRUE,
    col = mred,
    cex = 2)

```


```{r}
#| fig-height: 4
#| fig-width: 5 

lm_res = lm(z ~ x, data = d)

plot_d(paste0("Model predictive mean ❌"))

abline(a = lm_res$coefficients[1],
       b = lm_res$coefficients[2],
       col = mred,
       lwd = 1.8)

plt(nx, 
    predict(lm_res, data.frame(x = nx)),
    col = mred,
    add = TRUE,
    cex = 2)

```

:::{.fragment fragment-index=1}
```{r}
#| fig-height: 4
#| fig-width: 5 

plot_d("Multiple predictive mean + noise ✅")

lm_res = lm(z ~ x, data = d)

abline(a = lm_res$coefficients[1],
       b = lm_res$coefficients[2],
       col = mred,
       lwd = 1.8)

set.seed(12)

plt(nx, 
    predict(lm_res, data.frame(x = nx)) + rnorm(3*4, sd = summary(lm_res)$sigma),
    col = mred,
    add = TRUE,
    cex = 2)

```
:::

:::{.fragment fragment-index=2}
```{r}
#| fig-height: 4
#| fig-width: 5 
plot_d("Predict + noise + θ uncertainty ✅✅")

# rstanarm::stan_glm(z ~ x, data = d) |> rstanarm::as_draws() |> head() |> dput()

ddf = structure(list(`(Intercept)` = c(0.0471073690698166, 0.201052925741826, 
0.250345212091326, 0.198926964048982, 0.299822320222807, 0.0208228507399506
), x = c(0.93778290924944, 0.899190012816253, 0.918602409313972, 
0.743191159898737, 0.806488438565962, 0.959382089089642), sigma = c(0.757680208606442, 
0.973403819489721, 0.698234171286681, 0.782440029970045, 0.924800482357415, 
0.708375154404558)), row.names = c(NA, -6L), class = c( "data.frame"))

for (i in 1:4) {
  abline(a = ddf$`(Intercept)`[i],
       b = ddf$x[i],
       col = mred,
       lwd = 1.8)
  
  plt(nx, 
      rnorm(3, 
            mean = ddf$`(Intercept)`[i] + ddf$x[i]*nx, 
            sd = ddf$sigma[i]),
      col = mred,
      add = TRUE,
      cex = 2)
}
```
:::

:::


:::
::::

## MI - Analysis model

::::{.columns}
:::{.column width=55%}
* Fit the intended model of interest to each imputed dataset
* i.e. run your `t.test()` / `lm()` / `glm()` / etc as if the data were complete
* extract estimates & error bars from each

:::{.fragment}
* *All the usual model checking steps apply!*
:::

:::

:::{.column width=45%}

![](images/ch01-miflow-1.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" width=75%}

::: {data-id="box1" style="border: 5px solid #FF0000; width: 220px; height: 270px; " .absolute left=1032 top=115}
:::

:::
::::

## MI - Pooling

::::{.columns}
:::{.column width=55%}

Aggregate estimates in a way that combines:

1. within-imputation uncertainty
1. between-imputation uncertainty
1. uncertainty from finite imputations 

End result: *unbiased* and *confidence valid* estimates

:::{.fragment}

* `?mice::pool()`
* "Rubin's rules"
* FIMD §2.3 <https://stefvanbuuren.name/fimd/sec-whyandwhen.html>

:::

:::

:::{.column width=45%}

![](images/ch01-miflow-1.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" width=75%}

::: {data-id="box1" style="border: 5px solid #FF0000; width: 220px; height: 270px; " .absolute left=1170 top=115}
:::

:::
::::

## "Ignorability" assumption

> "Does the chance of a value going missing depend on information we don't have?"

* If so, assumption violated.
* Standard MI practice generally won't work.

## "Joint imputation"

:::{.column width=65%}
* Use *informative auxiliary variables* to simultaneously:
  * impute NAs
  * infer missingness relationship 
* Generally requires:
  * informative auxiliary variables
  * bespoke modeling in a probabilistic programming language 
  * fairly large sample sizes
  * domain knowledge
:::

![](images/covid_dag.png){fig-alt="" .absolute left=1100 top=100 width=30%}

<!-- TODO: think about the dag on the right more -->

## Quiz!

```{r}
ex_dat[1:7,c(2:4,1)] |> print(row.names = FALSE)
```

[<span style="color:#666666; font-style: italic;"> <small> Missing values in survey data </small> </span>]{.absolute left=25 top=540}

::: {data-id="box1" style="border: 2px solid #888888; 0px; background-color: #888888; height: 550px; " .absolute left=830 top=70} 
:::

![ ](images/dg_tmp.png){.absolute left=900 top=100 width=44%} 

[<span style="color:#666666; font-style: italic;"> <small> Missing values in imaging data. Image provided by HMS Single Cell Core. </small> </span>]{.absolute left=900 top=540}

:::{.absolute top=600}
**Discuss with your neighbor:** 

* Which are MCAR / MAR / NMAR?
* Which exhibit ignorable missingness? 
* If nonignorable, what extra info could rescue it?
:::

# Examples

## Highly accurate imputation - Genotype from markers

:::{.column width=60%}
* Input: microarray genotypes of 100k-5M common marker SNPs
* Output: genotypes at query SNPs
* Output uncertainty: surprisingly low for common variants
* SHAPEIT - Segmented HAPlotype Estimation and Imputation Tools
:::

:::{.column width=40% .absolute top=100 left=1000}
![](images/2025-09-19_10-23-13.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);"}
![](images/2025-08-04_13-22-07.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);"}
:::

<!-- TODO: picture of a microarray -->

## Imprecise imputation for images - single-cell proteomics

:::{.column width=50%}
* Proof of concept. N = 8.
* Elastic Net, XGBoost, AutoEncoders to impute 16 proteins of single cells.
* MAE b/w 0.05-0.15 on a 0-1 scale.
* Imputed error fairly high → Lessened downstream utility
* Batch, instrument effects? Unclear.
:::

:::{.column width=4%}
 
:::

:::{.column width=46%}
![](images/2025-08-04_12-36-50.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);"}
![](images/2025-08-04_12-38-36.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);"}
:::

## COVID incidence 

> "... a model that allows for simultaneous modeling of the disease and missingness processes, and that incorporates information on spatial clustering of risk"

> "population relative risk estimates by race during the early part of the COVID-19 pandemic in Michigan were understated for non-white residents, compared to white residents, when cases missing race were dropped or had these values imputed using MI."

![](images/2025-07-01_13-17-22.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" width=36% fig-align="center"} 

:::{.aside}
*Modeling racial/ethnic differences in COVID-19 incidence with covariates subject to nonrandom missingness* - Trangucci 2023
:::

# End matter

## Ethics

> "The ethical statistical practitioner ... seeks to *understand and mitigate* known or *suspected* limitations, defects, or biases in the data ... and communicates potential impacts on the interpretation [or] conclusions... [and] Avoids compromising validity for expediency." ^[ [ASA Ethical Guidelines for Statistical Practice](https://www.amstat.org/your-career/ethical-guidelines-for-statistical-practice),  emphasis mine]

* Be transparent. 
* Do your due diligence. 

So,

* Reasonable, not perfect.
* Opportunity to advance statistical practice in your field.
* Ask for help if you need it.

## Missing data analysis in Python

:::{.incremental}
* `statsmodels.imputation.mice` 
* [`miceforest`](https://miceforest.readthedocs.io/en/latest/) - possibly okay for small data if computational burden of RFs is acceptable
* Don't use `scikit-learn` for missing data analysis
  * encourages single imputation ❌
  * statistical malpractice
  * See also the infamous `sklearn.linear_model.LogisticRegression`
:::

## Registration inquiries

> ... how should we decide whether the missing data is a problem?

FIMD §6.1: 
"The imputation model should:

* account for the process that created the missing data,
* preserve the relations in the data, and
* preserve the uncertainty about these relations."

Ch. 5-6 practical advice on how to do / think about / **simulate** this. Also, influx/outflux.

> Survival analysis

## References

::::{.columns}
:::{.column width=55%}
* [Flexible Imputation of Missing Data](https://stefvanbuuren.name/fimd/), Stef van Buuren
  * [`mice` vignettes](https://amices.org/mice/index.html#vignettes)
* [Statistical Rethinking by Richard McElreath](https://xcelab.net/rm/statistical-rethinking/)
  * [Lecture series on YT (esp. 1-13)](https://www.youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus)
  * [brms + tidyverse translation of official text](https://bookdown.org/content/4857/)
:::

:::{.column width=10%}
 
:::
:::{.column width=35%}
![](images/2025-09-09_10-52-59.png){fig-alt="" style="filter: drop-shadow(0 0 0.75rem grey);" width=500}
:::
:::

# On to the workshop!

[ccb.connect.hms.harvard.edu/missing_data_workshop/](ccb.connect.hms.harvard.edu/missing_data_workshop/)

# Extra

## NA and friends

* "NA" - not applicable, not available, etc
* `NA` - logical constant in R
  * See `?NA` for `NA_real_`, `NA_character_`, etc
* NaN - 0/0 and so on
* `0` - Not the same as a missing value!
* Placeholders e.g. `age = 9999`
  * 🪦 

## {background-image="images/beach.png"} 

## Defining MCAR, MAR, NMAR

see 2.2.4 in FIMD: https://stefvanbuuren.name/fimd/sec-idconcepts.html

* $Y$ - values of interest, observed and missing 
* $\psi$ - missingness model parameters
* $R$ - indicator of a datum being present/absent (i.e. $R=0$ → observation is missing)

MCAR:

$$ P(R=0 \mid Y_{obs}, Y_{mis}, \psi) = P(R=0 \mid \psi)$$

MAR:

$$ P(R=0 \mid Y_{obs}, Y_{mis}, \psi) = P(R=0 \mid Y_{obs}, \psi)$$

MNAR → expression doesn't simplify:

$$ P(R=0 \mid Y_{obs}, Y_{mis}, \psi)$$ 

## Examples

* genotype impute
* scdropout
* COVID data
* find a good survey example

