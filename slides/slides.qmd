---
title: "Modern Strategies for Working with Missing Data"
subtitle: "with applications in biomedical research"
author: "Andrew Ghazi"
institute: "Core for Computational Biomedicine"
date: September 24, 2025 
format: 
    revealjs:
      theme: [default, my_theme.scss]
      width: 1600
      height: 900
      incremental: false 
      transition: fade
      background-transition: fade
      transition-speed: fast
      chalkboard: true
      slide-number: c/t
      controls: true
      controls-layout: edges
      controls-tutorial: true
        
title-slide-attributes:
  data-background-image: images/HMS_DBMI_Logo.png
  data-background-size: 25%
  data-background-position: 10% 10%
---

# Front matter 

## Setup {visibility="hidden"}

<!-- Repo: https://github.com/ccb-hms/missing_data/tree/main/slides -->
<!-- Rendered output: https://ccb.connect.hms.harvard.edu/missing_data -->

<!-- TODO better attribution on many of the figures -->

```{r setup}
#| message: false
library(fastverse)
library(tinyplot)
# library(ggplot2) # Exclude these, I want it to render fast
# library(ggbeeswarm)
# library(ggmice)
set.seed(123)

ow = "#F2F2F2"

# mice colors: 
# mice::mdc(r = c("observed", "missing")) |> dput()
mblue = "#006CC2B3"
mred = "#B61A51B3"

ow_rect = ggplot2::element_rect(fill  = ow,
                                color = ow) 

ow_bg = ggplot2::theme_minimal() + 
  ggplot2::theme(panel.background = ow_rect,
                 plot.background  = ow_rect,
                 legend.background = ow_rect,
                 text = ggplot2::element_text(size = 25))

set.seed(42)

n = 30

s = .06

d = data.frame(x = rnorm(n)) |> 
  mtt(y = rnorm(n, 1*x, sd = s), # small sd
      z = rnorm(n, 1*x, sd = 1)) # big sd

yr = range(c(d$y, d$z))
```

## CCB R/Stats Office Hours


:::{.column width="55%"}
* Quick help on R/Stats & CompBio
  * Finding data
  * Viz
  * Modeling
  * Programming
  * Debugging
* Book a timeslot, Zoom or in-person
  
  <!-- * Data wrangling / plotting messy health database exports -->
  <!-- * Prognostic modeling of surgery patients from imaging features with random forests -->
  <!-- * R project installation & organization -->
  <!-- * debugging AlphaFold3 on O2 -->
:::

![](images/oh.png){.absolute left=900 top=0 width=45% fig-alt="Office hours overview figure showing R package logos and statistical visualizations"}


:::{.absolute left=0 top=770}
Link in my email signature or at [https://dbmi.hms.harvard.edu/about-dbmi/core-computational-biomedicine/office-hours](https://dbmi.hms.harvard.edu/about-dbmi/core-computational-biomedicine/office-hours) 
:::

:::{.absolute left=1160 top=870 width=17%}
<span style="color: #999999;"> <small> R for Data Science (2e) [CC BY-NC-ND 3.0](https://creativecommons.org/licenses/by-nc-nd/3.0/us/) </small> </span>
:::

## Prerequisites:

* Seminar: none
* Hands-on workshop: 
  * statistics basics
  * R basics
  * R and the packages below:

```{r}
#| eval: false
#| echo: true

pkgs <- c("ggplot2", "dplyr", "purrr", 
          "mice", "ggmice", "brms")

install.packages(pkgs, Ncpus = 4)
```

## Outline 

:::{.column width=70%}
* Seminar (1 hour)
  * Concepts
  * Strategies
  * Examples
  * No theory or math!
* Hands-on workshop  in R (1 hour)

These slides:

[ccb.connect.hms.harvard.edu/missing_data](ccb.connect.hms.harvard.edu/missing_data)

:::

![](images/2025-06-11_15-35-18.png){fig-alt="A complicated looking missing data model diagram" style="filter: drop-shadow(0 0 0.75rem grey);" .absolute left=1000 top=50 width=33%}
![](images/no.svg){fig-alt="A big red circular NO icon on top" .absolute left=985 top=20 .fragment width=35%}

# Concepts 

## Missing data and NAs

:::{.column width=60%}
* NA = **N**ot **A**pplicable
  * *not* the same as NaN, `NULL`, etc
* "Missing data analysis"
* ubiquitous and important problem
* established methods/tools ~15 years
* not challenging, but requires statistics foundation

<!-- :::{.fragment} -->
<!-- * **BUT** a thorough overview requires advanced methods  -->

<!-- ![](images/m2.png){width=80% fig-align="center" style="filter: drop-shadow(0 0 0.75rem grey);"} -->
<!-- ::: -->

:::

:::{.absolute left=970 top=40 width=50%}
```{r}

ins_na = \(x) {
  x[sample(length(x), floor(n/3))] = NA
  x
}

n = 200
set.seed(123)
ex_dat = data.table(allerg       = sample(0:1, n, TRUE)   |> ins_na(),
                    age          = floor(runif(n, 30,80)) |> ins_na(),
                    income       = floor(rexp(n, 1/48))   |> ins_na(),
                    `5y_outcome` = sample(0:1, n, T)      |> ins_na())

ex_dat$income[1] = 20
ex_dat$allerg[2] = NA

ex_dat[1:5,-1] |> 
  print(row.names = FALSE)
```
:::

[<span style="color:#666666; font-style: italic;"> <small> Missing values in survey data </small> </span>]{.absolute left=1000 top=390}

![](images/dg_tmp.png){.absolute left=1000 top=460 width=40% fig-alt="A fluorescence image of cell nuclei of the dentate gyrus of a mouse brain. There are some black speckles in the densest region of the image."} 

[<span style="color:#666666; font-style: italic;"> <small> Missing values in imaging data. Image provided by HMS Single Cell Core.</small> </span>]{.absolute left=1000 top=870}

## Degrees of uncertainty

* Fill in the blank: 
    * "Harvard Uni_ersity"
    * "_ight" 


:::{.incremental}
1. high or low uncertainty over missing values
1. degree of uncertainty impacted by context
:::

:::{.fragment}
* *Imputation* - using context to fill in the blanks
:::

## Degrees of uncertainty 

::::{.columns}

:::{.column width=44%}
* Context informs uncertainty when imputing

<!-- :::{.fragment} -->
<!-- * If the uncertainty is *sufficiently* small, ignore it.  -->
<!-- * Using imputed values for...? -->
<!--     * Just reporting / visualizing. ‚úÖ -->
<!--     * Input to scientific inference. ‚ö†Ô∏è -->
<!-- ::: -->

:::

:::{.notes}
* People generally ascribe negative value to uncertainty
:::

:::{.column width=56%}

<small> *Missing Y values at X = 1:* </small>

```{r}
#| fig-alt: "A plot of two linear relationships of X and Y variables. The left has a strong correlation, right weak. There's vertical lines at X=1 on both. The left vertical line is small, the right is tall."

tinytheme("clean2", 
          mar = c(2,2,1,1),
          family = "Arial",
          bg = ow,
          cex.axis = 1.5,
          col.axis = "#222222")

par(mfrow = c(1,2))

plt(d$x, d$y, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 2,
    ylim = yr)

lines(col = mred, 
      c(1,1),
      c(1+s*2, 1-s*2),
      lend = "square", 
      lwd = 5)

# points(1, 1, col = "firebrick2", pch = 21, cex = 1.3)

plt(d$x, d$z, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 2,
    ylim = yr) 

lines(col = mred,
    c(1,1),
    c(1+1*2, 1-1*2),
    lend = "butt",
    lwd = 5)

# points(1, 1, col = "firebrick2", bg = "white", pch = 21, cex = 1.3)

```
:::
:::

## Imputation - "Filling in the blank"

::::{.columns}
:::{.column width=46%}
* Generate a new value to replace NAs
* Build a predictive model for variable with NAs from other variables

:::{.fragment fragment-index=1}
* Many pitfalls:
  * replacing with the mean ‚ùå 
  * replacing with one value ‚ùå
  * replacing with unrealistic values ‚ùå
:::
:::


:::{.column width=54% .fragment fragment-index=1}

<small> *Given X = (-2, -1, 1), predict missing Y values:* </small>

```{r}
#| fig-alt: "A plot of two linear relationships with imputed values at X=(-2,-1,1). The y values are imputed as the mean value of y in the left panel, and are placed on a linear fit on the right."
par(mfrow = c(1,2))

plt(d$x, d$z, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 2,
    ylim = yr)

legend(-2.6, 2.8,
       c("observed", "imputed Y"),
       col = c(mblue, mred),
       cex = 1.4,
       pt.cex = 2,
       bg = "#F2F2F2",
       box.col = "#222222",
       text.col = "#222222",
       pch = c(16, 16))

my = mean(d$y)

nx = c(-2, -1,1)

plt(nx,
    rep(my, 3),
    add = TRUE,
    col = mred,
    cex = 3)

lm_res = lm(z ~ x, data = d)

plt(d$x, d$z, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 2,
    ylim = range(c(d$y, d$z)))

abline(a = lm_res$coefficients[1],
       b = lm_res$coefficients[2],
       col = mred)

# plt(type = "l",
#     col = mred,
#     )

plt(nx, 
    predict(lm_res, data.frame(x = nx)),
    col = mred,
    add = TRUE,
    cex = 3)
```
:::
::::

## Uses for imputed values


- Fill in empty regions on a figure
  - UMAP or image
- Downstream scientific inference
  - $P(\theta \mid Y_{imp})$
  
> *Are the imputed values the means or the ends?*

***Inference is much more demanding!***

![](images/red_dotted.png){fig-alt="A single cell UMAP plot. There's a dotted red oval where a putative missing cluster might go." style="filter: drop-shadow(0 0 0.75rem grey);" .absolute left=900 top=0 width=36%}
![](images/prop_ex.png){fig-alt="A linear regression plot with observations, imputed missing values, and a regression line with a bowtie shaped uncertainty ribbon" style="filter: drop-shadow(0 0 0.75rem grey);" .absolute left=900 top=450 width=36%}

## Undesirable effects of missing data

* Obvious: Missing data ‚Üí less information ‚Üí learn less 

:::{.fragment fragment-index=1}

* Subtle yet harmful: Patterns of missingness can *induce* bias if ignored

```{r}
#| eval: false

# tinytheme("clean2",
#           mar = c(2,2,1,1),
#           bg = ow,
#           cex.axis = 1.25,
#           col.axis = "#222222",
#           mfrow = c(1,3))

tinytheme("clean2", 
          mar = c(2,2,4,1),
          bg = ow,
          family = "Arial",
          cex.axis = 1.5,
          cex.main = 1.5,
          col.main = "#222222",
          col.axis = "#222222")

png("slides/images/patterns.png", width = 3*1500, height = 1250, units = "px",
    res = 400)
par(mfrow = c(1,4)) # mfrow behaves oddly within tinytheme

set.seed(1) 

n = 150

d2 = data.table(x = rnorm(n)) |> 
  mtt(latent = rnorm(n, mean = 1*x, sd = 1.2),
      eta_z =    4.5*(latent - x) + -7*(x+1), 
      eta_w = -1.5*(latent - x) + -3*x - 1.6,
      z_mis = as.logical(rbinom(n, 1, plogis(eta_z))) & (x < 0) | 
               (x < -.33 & latent > -.5) | 
               (x < -.5 & latent > -1) | 
        (x < -.7 & latent > -1.5 ),
      w_mis = as.logical(rbinom(n, 1, plogis(eta_w))),
      r_mis = !(latent < (-1*x + 1.33) & latent > (-1*x -.75)),
      z = data.table::fcase(z_mis, NA_real_,
                            !z_mis, latent),
      w = data.table::fcase(w_mis, NA_real_,
                           !w_mis, latent),
      r = data.table::fcase(r_mis, NA_real_,
                            !r_mis, latent))

# d2 |> ggplot(aes(x, latent)) + geom_point(aes(color = eta_z)) + scale_color_viridis_c()

xr2 = range(d2$x); yr2 = range(d2$latent)

plt(d2$x, d2$latent, 
    xlim = xr2, ylim = yr2,
    # col = parula[as.numeric(cut(breaks = 100, d2$eta_z))],
    col = "grey44",
    pch = 16,
    main = "latent truth",
    cex = 1.7)

abline(reg = lm(latent ~ x, data = d2), col = "grey10")

plt(d2$x, d2$z, 
     xlim = xr2, ylim = yr2,
     pch = 16,
     col = mblue,
    main = "missingness increases slope",
    cex = 1.7)

abline(reg = lm(z ~ x, data = d2),
       col = mblue)

zm = is.na(d2$z)

points(d2$x[zm], d2$latent[zm],
       pch = 1,
       col = mred,
       cex = 1.7)

legend(-2.3, 5.2,
       c("missing", "observed"),
       col = c(mred, mblue),
       cex = 1.4,
       pt.cex = 2,
       bg = "#F2F2F2",
       box.col = "#222222",
       text.col = "#222222",
       pch = c(1, 16))

plt(d2$x, d2$w, 
     xlim = xr2, ylim = yr2,
     pch = 16,
     col = mblue,
    main = "missingness decreases slope",
    cex = 1.7)

abline(reg = lm(w ~ x, data = d2),
       col = mblue)

wm = is.na(d2$w)

points(d2$x[wm], d2$latent[wm],
       pch = 1,
       col = mred,
       cex = 1.7)

plt(d2$x, d2$r, 
     xlim = xr2, ylim = yr2,
     pch = 16,
     col = mblue,
    main = "missingness flips slope",
    cex = 1.7)

abline(reg = lm(r ~ x, data = d2),
       col = mblue)

rm = is.na(d2$r)

points(d2$x[rm], d2$latent[rm],
       pch = 1,
       col = mred,
       cex = 1.7)
dev.off()
tinytheme("clean2", 
          mar = c(2,2,1,1),
          bg = ow,
          cex.axis = 1.25,
          col.axis = "#222222")


```


![](images/patterns.png){fig-alt="A plot of four datasets. The first is the latent true relationship, the second has a missingness pattern that increases the slope of the line, the third decreases the slope, and the fourth makes the slope negative."}

:::


::: {data-id="box1" style="background-color: #F2F2F2; width: 1600px; height: 720px; " .absolute left=370 top=300 .fragment .fade-out fragment-index=2}
:::

::: {data-id="box1" style="background-color: #F2F2F2; width: 1600px; height: 720px; " .absolute left=750 top=300 .fragment .fade-out fragment-index=3}
:::

## Three types of missingness

::::{.columns}
:::{.column width=50%}
* Missing...
  * "...completely at random"
  * "...at random" 
  * "...not at random"
  <br></br>
  
:::

:::{.column .fragment fragment-index=2 width=50%} 
* These terms are famously confusing.
* Draw a directed acyclic graph (DAG) of your system.
:::

::::

![](images/ya_like_dags.png){fig-align="center" width=90% fig-alt="Three causal diagrams with emoji demonstrating MCAR, MAR, and MNAR."}

:::{.absolute left=50 top=875}
<small> Arrows: causal relationships. Dotted: latent true value. Starred: observations with NAs. </small>
:::

:::{.absolute top=875 left=1140}
<span style="color: #999999;"> <small> Adapted from *Statistical Rethinking* </small> </span>
:::

::: {data-id="box1" style="background-color: rgba(242, 242, 242, 0.5); backdrop-filter: blur(15px); width: 1960px; height: 475px; " .fragment .fade-out fragment-index=1 .absolute left=470 top=400}
:::

# Strategies

## Examining missingness

* Count observations with any NAs
* Tabulate co-missingness 
* Check if NAs are associated with other variables

:::{layout-nrow=1}
![](images/ggmice_preview2.png){height=520 fig-alt="A preview image of the plot pattern diagnostics from ggmice"}
![](images/ggmice_preview.png){height=520 fig-alt="A preview image of from a scatterplot with missing values ggmice"}
:::

```{r}
#| eval: false 
#| echo: false

ggmice(boys, aes(age, hgt)) + 
  geom_point(size = 3, pch = 16) + 
  theme_minimal() +
  theme(panel.background = ow_rect, 
        panel.grid = element_blank(),
        plot.background = ow_rect,
        text = element_text(size = 18, family = "Arial")) +
  labs(y = "height")

ggsave("~/projects/missing_data/slides/images/ggmice_preview.png", h = 5, w = 7.5)

plot_pattern(boys)  + 
  theme(axis.text = element_text(hjust = .5, vjust = .5),
        panel.background = ow_rect, 
        panel.grid = element_blank(),
        plot.background = ow_rect)
```


## Just drop them

* `na.omit()`
* Only valid for MCAR, otherwise you'll likely get bias.
* At the very least:
  * Characterize dropped cases
  * Justify and discuss the consequences of this choice

## Multiple imputation (MI)

* Key idea: 
  1. Randomly impute missing values
  1. Fit a model to the imputed dataset
  1. Repeat 1-2 many times
  1. Aggregate ("pool") results
  
* Key assumption: "ignorability"
* Core package: `mice`

![](images/ch01-miflow-1.png){.absolute top=100 left=750 fig-alt="The MI overview figure from the textbook. There's a main circle representing the incomplete data. It expands to three parallel paths that are imputed complete datasets that are analyzed. The paths then collapse together again for the pooled result." style="filter: drop-shadow(0 0 0.75rem grey);" width=55%}

:::{.aside}
[*Flexible Imputation of Missing Data*](https://stefvanbuuren.name/fimd/), Stef van Buuren, [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)
:::

## MI - Imputation model

::::{.columns} 

:::{.column width=45%}


![](images/ch01-miflow-1.png){fig-alt="The same MI diagram with the imputation step highlighted" style="filter: drop-shadow(0 0 0.75rem grey);" width=75%}

::: {data-id="box1" style="border: 5px solid #FF0000; width: 220px; height: 270px; " .absolute left=20 top=115}
:::

* Predict NAs in a variable given others

[<small> *Given X = (-2, -1, 1), predict missing Y values:* </small>]{.absolute left=750 top=50}

:::{.fragment fragment-index=4}
* LASSO, random forests, neural nets
* Predictive mean matching ‚úÖ‚úÖ
* See `?mice::mice.impute.*`
* `mice` uses sane defaults, but always check and think.
:::

:::

<!-- FIMD figure 3.1 but with only one predictor for simplicity -->
```{r}
plot_d = function(l) {
  plt(d$x, d$z, 
    xlab = "", 
    ylab = "", 
    col = mblue,
    cex = 1.8,
    ylim = yr)
  
  pd = .2
  
  cex_val = 1.33
  
  w = strwidth(l, cex = cex_val)
  h = strheight(l, cex = cex_val)
  
  x = -2.65
  y = 2.7
  
  plt(type = "rect",
      fill = 'grey90',
      add = TRUE,
      col = rgb(0,0,0,0),
      xmin = x - pd, xmax = x + w + pd,
      ymin = y - pd, ymax = y + h + pd)
  
  plt(type = "text",
      x = x + w/2, y = y + h/2,
      labels = l,
      col = "grey20",
      cex = cex_val,
      add = TRUE)

}

```

:::{.column width=55%}

:::{#figs layout-ncol=2}


```{r .fragment}
#| fig-height: 4
#| fig-width: 5 
#| fig-alt: "And imputation model that imputes with the mean"

# TODO prerender these, because the EKS server's R can't find the emojis in the font :( 

plot_d("Mean of the non-NAs ‚ùå‚ùå")

my = mean(d$y)

nx = c(-2, -1,1)

plt(nx,
    rep(my, 3),
    add = TRUE,
    col = mred,
    cex = 2)

```


```{r}
#| fig-height: 4
#| fig-width: 5 
#| fig-alt: "An imputation model that places the data on a linear fit"

lm_res = lm(z ~ x, data = d)

plot_d(paste0("Linear mean ‚ùå"))

abline(a = lm_res$coefficients[1],
       b = lm_res$coefficients[2],
       col = mred,
       lwd = 1.8)

plt(nx, 
    predict(lm_res, data.frame(x = nx)),
    col = mred,
    add = TRUE,
    cex = 2)

```

:::{.fragment fragment-index=1}
```{r}
#| fig-height: 4
#| fig-width: 5 
#| fig-alt: "An imputation model that adds multiple imputations with stochastic noise added."

plot_d("Linear mean + noise ‚úÖ")

lm_res = lm(z ~ x, data = d)

abline(a = lm_res$coefficients[1],
       b = lm_res$coefficients[2],
       col = mred,
       lwd = 1.8)

set.seed(12)

plt(nx, 
    predict(lm_res, data.frame(x = nx)) + rnorm(3*4, sd = summary(lm_res)$sigma),
    col = mred,
    add = TRUE,
    cex = 2)

```
:::

:::{.fragment fragment-index=2}
```{r}
#| fig-height: 4
#| fig-width: 5 
#| fig-alt: "And imputation model building on the last one that also takes random posterior draws of the linear fit itself"

plot_d("Line + noise + Œ∏ uncertainty ‚úÖ‚úÖ")

# rstanarm::stan_glm(z ~ x, data = d) |> rstanarm::as_draws() |> head() |> dput()

ddf = structure(list(`(Intercept)` = c(0.0471073690698166, 0.201052925741826, 
0.250345212091326, 0.198926964048982, 0.299822320222807, 0.0208228507399506
), x = c(0.93778290924944, 0.899190012816253, 0.918602409313972, 
0.743191159898737, 0.806488438565962, 0.959382089089642), sigma = c(0.757680208606442, 
0.973403819489721, 0.698234171286681, 0.782440029970045, 0.924800482357415, 
0.708375154404558)), row.names = c(NA, -6L), class = c( "data.frame"))

for (i in 1:4) {
  abline(a = ddf$`(Intercept)`[i],
       b = ddf$x[i],
       col = mred,
       lwd = 1.8)
  
  plt(nx, 
      rnorm(3, 
            mean = ddf$`(Intercept)`[i] + ddf$x[i]*nx, 
            sd = ddf$sigma[i]),
      col = mred,
      add = TRUE,
      cex = 2)
}
```
:::

:::


:::
::::

## MI - Analysis model

::::{.columns}
:::{.column width=55%}
* Fit the intended model of interest to each imputed dataset
* i.e. run your `t.test()` / `lm()` / `glm()` / etc as if the data were complete
* extract estimates & error bars from each

:::{.fragment}
* *All the usual model checking steps apply!*
:::

:::

:::{.column width=45%}

![](images/ch01-miflow-1.png){fig-alt="The same MI overview figure with the analysis step highlighted" style="filter: drop-shadow(0 0 0.75rem grey);" width=75%}

::: {data-id="box1" style="border: 5px solid #FF0000; width: 220px; height: 270px; " .absolute left=1032 top=115}
:::

:::
::::

## MI - Pooling

::::{.columns}
:::{.column width=55%}

Aggregate estimates in a way that combines:

1. within-imputation uncertainty
1. between-imputation uncertainty
1. uncertainty from finite imputations 

End result: *unbiased* and *confidence valid* estimates

:::{.fragment}

* `?mice::pool()`
* FIMD ¬ß2.3 <https://stefvanbuuren.name/fimd/sec-whyandwhen.html>

:::

:::

:::{.column width=45%}

![](images/ch01-miflow-1.png){fig-alt="The same MI overview figure with the pooling step highlighted" style="filter: drop-shadow(0 0 0.75rem grey);" width=75%}

::: {data-id="box1" style="border: 5px solid #FF0000; width: 220px; height: 270px; " .absolute left=1170 top=115}
:::

:::
::::

## "Ignorability" assumption

> "Does the chance of a value going missing depend on information we don't have?"

* If so, assumption violated.
* Standard MI practice generally won't work.

## "Joint imputation"

:::{.column width=65%}
* Use *informative auxiliary variables* to simultaneously:
  * impute NAs
  * infer missingness relationship 
  
:::{.fragment}
* Generally requires:
  * informative auxiliary variables
  * custom model in a probabilistic programming language 
  * fairly large sample sizes
  * domain knowledge
:::

:::

![](images/covid_dag.png){fig-alt="An oversimplified DAG of the social/geographic COVID model" .absolute left=1100 top=100 width=30%}

:::{.absolute left=750 top=870}
<span style="color:#777777;"> <small> Arrows: causal relationships, Dotted: latent true value, Starred: value with NAs </small> </span>
:::

## Quiz!

```{r}
ex_dat[1:7,c(2:4,1)] |> print(row.names = FALSE)
```

[<span style="color:#666666; font-style: italic;"> <small> Missing values in survey data </small> </span>]{.absolute left=25 top=540}

::: {data-id="box1" style="border: 2px solid #888888; 0px; background-color: #888888; height: 570px; " .absolute left=830 top=60} 
:::

![ ](images/dg_tmp.png){.absolute left=900 top=100 width=44% fig-alt="The same dentate gyrus image from earlier"} 

[<span style="color:#666666; font-style: italic;"> <small> Missing values in imaging data. Image provided by HMS Single Cell Core. </small> </span>]{.absolute left=900 top=540}

:::{.absolute top=600}
**Discuss with your neighbor:** 

* Which are MCAR / MAR / MNAR?
* Which exhibit ignorable missingness? 
* If nonignorable, what extra info could rescue it?
:::

# Examples

## Highly accurate imputation - Genotype from markers

:::{.column width=60%}
* Input: microarray genotypes of 100k-5M common marker SNPs
* Output: genotypes at query SNPs
* Output uncertainty: surprisingly low for common variants
* SHAPEIT - Segmented HAPlotype Estimation and Imputation Tools
:::

:::{.column width=40% .absolute top=100 left=1000}
![](images/Microarray2.gif){fig-alt="microarray diagram" style="filter: drop-shadow(0 0 0.75rem grey);"} 
![](images/2025-08-04_13-22-07.png){fig-alt="A figure from the shapeit paper. It shows how imputation R2 increases from .2 to nearly 1 as a function of increasing minor allele count in the reference panel from singleton to 100k+ MAC." style="filter: drop-shadow(0 0 0.75rem grey);"}
:::

![](images/2025-09-19_10-23-13.png){fig-alt="A relevant paper: Genotype imputation using the Positional Burrows Wheeler Transform, doi: 10.1371/journal.pgen.1009049" style="filter: drop-shadow(0 0 0.75rem grey);" .absolute left=100 width=45% top=600}

## Imprecise imputation for images - single-cell proteomics

:::{.column width=50%}
* Proof of concept. N = 8.
* Elastic Net, XGBoost, AutoEncoders to impute 16 proteins of single cells.
* MAE b/w 0.05-0.15 on a 0-1 scale.
* Imputed error fairly high ‚Üí Lessened downstream utility
* Batch, instrument effects? Unclear.
:::

:::{.column width=4%}
 
:::

:::{.column width=46%}
![](images/2025-08-04_12-36-50.png){fig-alt="Paper header: Imputing single-cell protein abundance in multiplex tissue imaging doi: 10.1038/s41467-025-59788-x" style="filter: drop-shadow(0 0 0.75rem grey);"}
![](images/2025-08-04_12-38-36.png){fig-alt="A figure from the paper showing filled in protein expression images" style="filter: drop-shadow(0 0 0.75rem grey);"}
:::

## COVID incidence 

> "... a model that allows for simultaneous modeling of the disease and missingness processes, and that incorporates information on spatial clustering of risk"

> "population relative risk estimates by race during the early part of the COVID-19 pandemic in Michigan were understated for non-white residents, compared to white residents, when cases missing race were dropped or had these values imputed using MI."

![](images/2025-07-01_13-17-22.png){fig-alt="An interval plot contrasting relative risk estimates across different COVID incidence models" style="filter: drop-shadow(0 0 0.75rem grey);" .absolute top=440 width=44% left=450} 

:::{.aside}
*Modeling racial/ethnic differences in COVID-19 incidence with covariates subject to nonrandom missingness* - Trangucci 2023
:::

# End matter

## Ethics

> "The ethical statistical practitioner ... seeks to *understand and mitigate* known or *suspected* limitations, defects, or biases in the data ... and communicates potential impacts on the interpretation [or] conclusions... [and] Avoids compromising validity for expediency." ^[ [ASA Ethical Guidelines for Statistical Practice](https://www.amstat.org/your-career/ethical-guidelines-for-statistical-practice),  emphasis mine]

* Be transparent. 
* Do your due diligence. 

So,

* Reasonable, not perfect.
* Opportunity to advance statistical practice in your field.
* Ask for help if you need it.

## Missing data analysis in Python

:::{.incremental}
* `statsmodels.imputation.mice` 
* [`miceforest`](https://miceforest.readthedocs.io/en/latest/) - possibly okay for small data if computational burden of RFs is acceptable
* Don't use `scikit-learn` for missing data analysis
  * encourages single imputation ‚ùå
  * statistical malpractice
  * See also the infamous `sklearn.linear_model.LogisticRegression`
:::

## Registration inquiries

> ... how should we decide whether the missing data is a problem?

* Ch. 5-6 practical advice on how to do / think about / **simulate** this. Also, influx/outflux.
* FIMD ¬ß6.1: 

"The imputation model should:

* account for the process that created the missing data,
* preserve the relations in the data, and
* preserve the uncertainty about these relations."


> Survival analysis

[Rethinking Ch11 example](https://bookdown.org/content/4857/god-spiked-the-integers.html#bonus-survival-analysis)

![](images/survival_ex.png){fig-alt="A demonstrative figure showing the different time to event distributions for cats at a shelter that were or were not censored. The mean is higher for the uncensored group." style="filter: drop-shadow(0 0 0.75rem grey);" .absolute left=600 top=650 width=35%}

## References

::::{.columns}
:::{.column width=55%}
* [Flexible Imputation of Missing Data](https://stefvanbuuren.name/fimd/), Stef van Buuren
  * [`mice` vignettes](https://amices.org/mice/index.html#vignettes)
* [Statistical Rethinking by Richard McElreath](https://xcelab.net/rm/statistical-rethinking/)
  * [Lecture series on YT (esp. 1-13, 18)](https://www.youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus)
  * [brms + tidyverse translation of official text](https://bookdown.org/content/4857/)
:::

:::{.column width=10%}
 
:::
:::{.column width=35%}
![](images/2025-09-09_10-52-59.png){fig-alt="The cover of FIMD" style="filter: drop-shadow(0 0 0.75rem grey);" width=500}
:::
:::

# On to the workshop!

[ccb.connect.hms.harvard.edu/missing_data_workshop/](ccb.connect.hms.harvard.edu/missing_data_workshop/)

# Extra

## NA and friends

* "NA" - not applicable, not available, etc
* `NA` - logical constant in R
  * See `?NA` for `NA_real_`, `NA_character_`, etc
* NaN - 0/0 and so on
* `0` - Not the same as a missing value!
* Placeholders e.g. `age = 9999`
  * ü™¶ 

## {background-image="images/beach.png"} 



## Defining MCAR, MAR, NMAR

see 2.2.4 in FIMD: https://stefvanbuuren.name/fimd/sec-idconcepts.html

* $Y$ - values of interest, observed and missing 
* $\psi$ - missingness model parameters
* $R$ - indicator of a datum being present/absent (i.e. $R=0$ ‚Üí observation is missing)

MCAR:

$$ P(R=0 \mid Y_{obs}, Y_{mis}, \psi) = P(R=0 \mid \psi)$$

MAR:

$$ P(R=0 \mid Y_{obs}, Y_{mis}, \psi) = P(R=0 \mid Y_{obs}, \psi)$$

MNAR ‚Üí expression doesn't simplify:

$$ P(R=0 \mid Y_{obs}, Y_{mis}, \psi)$$ 

